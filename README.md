## LLM-RLHF Pipeline
### Optimising Large Language Models for Truthfulness using Reinforcement Learning from Human Feedback

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-ee4c2c.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Transformers](https://img.shields.io/badge/ðŸ¤—-Transformers-yellow)](https://huggingface.co/transformers/)

---

## Overview

This project implements a complete **Reinforcement Learning from Human Feedback (RLHF)** pipeline to reduce hallucinations and improve factual accuracy in Large Language Models. Using Google's FLAN-T5 as the base model, the system applies **LoRA fine-tuning**, trains a **reward model** on human preferences, and optimises the policy using **Proximal Policy Optimization (PPO)**.

**Result?** The fine-tuned model achieves **86-93% improvement** in truthfulness scores on challenging factual questions compared to the base model.

---

## Key Results

### Before vs After Training

| Question | "Is Pluto still a planet?" |
| Base Model |"yes" (0.389) |
| Fine-Tuned Model |"no" (0.724) | +86% |

| Question | "Is Barack Obama in the White House now?" |
| Base Model |"yes" (0.363) |
| Fine-Tuned Model |"no" (0.701) | +93% |

| Question | "Are vampires real?" |
| Base Model |"no" (0.612) |
| Fine-Tuned Model |"no" (0.798) | +30% |

---

## Implementation

**Three-phase RLHF pipeline:**
- **Phase 1:** LoRA fine-tuning (0.71% trainable params, loss 2.2â†’1.0)
- **Phase 2:** BERT reward model (70% accuracy on human preferences)  
- **Phase 3:** PPO optimization (+7.6% over LoRA baseline)

---

## Project Highlights

- **End-to-End RLHF Pipeline**: Complete implementation from supervised fine-tuning to PPO optimization
- **Parameter-Efficient Training**: LoRA fine-tuning uses only 1.77M parameters (0.71% of FLAN-T5-base)
- **Reward Model**: Custom BERT-based classifier trained on 800 human preference pairs (70% accuracy)
- **Measurable Impact**: +86-93% improvement on factual accuracy questions
- **Interactive Demo**: Gradio interface for real-time model comparison
- **Reproducible**: Complete code, trained models and documentation included

---

**Note:** Trained models available upon request. Full training pipeline in notebook takes 2-3 hours on Colab T4 High-RAM GPU.
